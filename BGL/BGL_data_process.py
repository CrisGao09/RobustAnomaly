import sys

sys.path.append('../')

import os
import gc
import pandas as pd
import numpy as np
from logparser import Spell, Drain
import argparse
from tqdm import tqdm
from logdeep.dataset.session import sliding_window
import json

tqdm.pandas()
pd.options.mode.chained_assignment = None

PAD = 0
UNK = 1
START = 2

# data_dir = os.path.expanduser("~/.dataset/bgl")
data_dir = os.path.expanduser("~/Documents/ADdatasets/bgl")
output_dir = "../output/bgl/"
log_file = "BGL.log"
log_templates_file = output_dir + "BGL.log_templates.csv"


# map the event id generated by drain to a simpler incrementing number
def mapping(df):
    log_temp = pd.read_csv(log_templates_file)
    log_temp.sort_values(by=["Occurrences"], ascending=False, inplace=True)
    log_temp_dict = {event: idx + 1 for idx, event in enumerate(list(log_temp["EventId"]))}
    print(log_temp_dict)
    with open(output_dir + "BGL.log_templates.json", "w") as f:
        json.dump(log_temp_dict, f)

    df["EventId"] = df["EventId"].apply(lambda x: log_temp_dict.get(x, -1))
    return df


# In the first column of the log, "-" indicates non-alert messages while others are alert messages.
def count_anomaly():
    total_size = 0
    normal_size = 0
    with open(data_dir + log_file, encoding="utf8") as f:
        for line in f:
            total_size += 1
            if line.split(' ', 1)[0] == '-':
                normal_size += 1
    print("total size {}, abnormal size {}".format(total_size, total_size - normal_size))


def deeplog_file_generator(filename, df, features):
    with open(filename, 'w') as f:
        for _, row in df.iterrows():
            f.write(str(row["Label"]) + ",")
            f.write(" ".join(str(v) for v in row["tokenLabel"]) + ",")
            for val in zip(*row[features]):
                f.write(','.join([str(v) for v in val]) + ' ')
            f.write('\n')


def parse_log(input_dir, output_dir, log_file, parser_type):
    log_format = '<Label> <Id> <Date> <Code1> <Time> <Code2> <Component1> <Component2> <Level> <Content>'
    regex = [
        r'(0x)[0-9a-fA-F]+',  # hexadecimal
        r'\d+.\d+.\d+.\d+',
        # r'/\w+( )$'
        r'\d+'
    ]
    keep_para = True
    if parser_type == "drain":
        # the hyper parameter is set according to http://jmzhu.logpai.com/pub/pjhe_icws2017.pdf
        st = 0.3  # Similarity threshold
        depth = 3  # Depth of all leaf nodes
        parser = Drain.LogParser(log_format, indir=input_dir, outdir=output_dir, depth=depth, st=st, rex=regex,
                                 keep_para=keep_para)
        parser.parse(log_file)
    elif parser_type == "spell":
        tau = 0.55
        parser = Spell.LogParser(indir=data_dir, outdir=output_dir, log_format=log_format, tau=tau, rex=regex,
                                 keep_para=keep_para)
        parser.parse(log_file)


#
# def merge_list(time, activity):
#     time_activity = []
#     for i in range(len(activity)):
#         temp = []
#         assert len(time[i]) == len(activity[i])
#         for j in range(len(activity[i])):
#             temp.append(tuple([time[i][j], activity[i][j]]))
#         time_activity.append(np.array(temp))
#     return time_activity

def preprocessing():
    #
    #
    # parser = argparse.ArgumentParser()
    # parser.add_argument('-p', default=None, type=str, help="parser type")
    # parser.add_argument('-w', default='T', type=str, help='window size(mins)')
    # parser.add_argument('-s', default='1', type=str, help='step size(mins)')
    # parser.add_argument('-r', default=0.4, type=float, help="train ratio")
    # args = parser.parse_args()
    # print(args)
    #

    ##########
    # Parser #
    #########

    # parse_log(data_dir, output_dir, log_file, 'drain')

    #########
    # Count #
    #########
    # count_anomaly()

    ##################
    # Transformation #
    ##################
    # mins
    window_size = 5
    step_size = 1
    train_ratio = 0.01
    dirty_ratio = 0.05

    df = pd.read_csv(f'{output_dir}{log_file}_structured.csv')

    # data preprocess
    df['datetime'] = pd.to_datetime(df['Time'], format='%Y-%m-%d-%H.%M.%S.%f')
    df["Label"] = df["Label"].apply(lambda x: int(x != "-"))
    df['timestamp'] = df["datetime"].values.astype(np.int64) // 10 ** 9
    df['deltaT'] = df['datetime'].diff() / np.timedelta64(1, 's')
    df['deltaT'].fillna(0)
    # convert time to UTC timestamp
    # df['deltaT'] = df['datetime'].apply(lambda t: (t - pd.Timestamp("1970-01-01")) // pd.Timedelta('1s'))

    # sampling with fixed window
    # features = ["EventId", "deltaT"]
    # target = "Label"
    # deeplog_df = deeplog_df_transfer(df, features, target, "datetime", window_size=args.w)
    # deeplog_df.dropna(subset=[target], inplace=True)

    # map event id to pure numbers
    df = mapping(df)
    df.to_csv(f'{output_dir}{log_file}_structured_mapped.csv')


def generate_comparison_datasets(window_size, step_size, train_ratio, dirty_ratio):
    noisy_data_path = output_dir+"datasets/noisy/"
    clean_data_path = output_dir+"datasets/clean/"
    if not os.path.exists(noisy_data_path):
        os.makedirs(noisy_data_path)
    if not os.path.exists(clean_data_path):
        os.makedirs(clean_data_path)

    df = pd.read_csv(f'{output_dir}{log_file}_structured_mapped.csv')
    # sampling with sliding window
    deeplog_df = sliding_window(df[["timestamp", "Label", "EventId", "deltaT"]],
                                para={"window_size": int(window_size) * 60, "step_size": int(step_size) * 60}
                                )  # window size = 5 min, step size 1 min

    df_abnormal = deeplog_df[deeplog_df["Label"] == 1]
    df_normal = deeplog_df[deeplog_df["Label"] == 0]
    print("abonormal data: " + str(df_abnormal.shape[0]))
    #########
    # Train #
    #########

    df_normal = df_normal.sample(frac=1, random_state=12).reset_index(drop=True)  # shuffle
    normal_len = len(df_normal)
    train_len = int(normal_len * train_ratio)
    dirty_len = int(train_len * dirty_ratio)
    if dirty_len>=df_abnormal.shape[0]:
        print("Error! noist data is more than abnormal data!")

    train_dirty = df_normal[:train_len - dirty_len]  # train dataset of dirty
    train_dirty = train_dirty.append(df_abnormal[:dirty_len])
    train_dirty = train_dirty.sample(frac=1, random_state=12)
    deeplog_file_generator(os.path.join(noisy_data_path, 'train'), train_dirty, ["EventId"])
    print("training size {} of dirty dataset".format(len(train_dirty)))

    train_clean = df_normal[:train_len]  # train dataset of clean
    train_clean = train_clean.sample(frac=1, random_state=12)
    deeplog_file_generator(os.path.join(clean_data_path, 'train'), train_clean, ["EventId"])
    print("training size {} of clean dataset".format(len(train_clean)))


    ##############
    # Test Normal #
    ##############
    test_normal_dirty = df_normal[train_len-dirty_len:]
    deeplog_file_generator(os.path.join(noisy_data_path, 'test_normal'), test_normal_dirty, ["EventId"])
    print("test normal size {} of dirty dataset".format(len(test_normal_dirty)))
    test_normal_clean = df_normal[train_len:]
    deeplog_file_generator(os.path.join(clean_data_path, 'test_normal'), test_normal_clean, ["EventId"])
    print("test normal size {} of clean dataset".format(len(test_normal_clean)))



    del df_normal
    del train_dirty
    del train_clean
    del test_normal_dirty
    del test_normal_clean
    gc.collect()

    #################
    # Test Abnormal #
    #################

    test_abnormal_dirty = df_abnormal[dirty_len:]
    deeplog_file_generator(os.path.join(noisy_data_path, 'test_abnormal'), test_abnormal_dirty, ["EventId"])
    print("test abnormal size {} of dirty dataset".format(len(test_abnormal_dirty)))

    deeplog_file_generator(os.path.join(clean_data_path, 'test_abnormal'), df_abnormal, ["EventId"])
    print("test abnormal size {} of clean dataset".format(len(df_abnormal)))



if __name__ == "__main__":
    generate_comparison_datasets(5, 1, 0.4, 0.05)